{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np import pandas as pd import warnings import\n",
    "matplotlib.pyplot as plt import seaborn as sns import joblib import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV from\n",
    "sklearn.metrics import accuracy_score, classification_report,\n",
    "confusion_matrix from sklearn.metrics import precision_score,\n",
    "recall_score from sklearn.preprocessing import StandardScaler,\n",
    "LabelEncoder from sklearn.ensemble import RandomForestClassifier from\n",
    "sklearn.pipeline import Pipeline\n",
    "\n",
    "warnings.filterwarnings(‘ignore’)\n",
    "\n",
    "def print_results(results): print(“BEST PARAMS:\n",
    "{}”.format(results.best_params\\_)) means =\n",
    "results.cv_results\\_\\[‘mean_test_score’\\] stds =\n",
    "results.cv_results\\_\\[‘std_test_score’\\] for mean, std, params in\n",
    "zip(means, stds, results.cv_results\\_\\[‘params’\\]): print(“{} (+/-{})\n",
    "for {}”.format(round(mean, 3), round(std \\* 2, 3), params))\n",
    "\n",
    "def evaluate_model(model, features, labels): start = time.time()\n",
    "predictions = model.predict(features) end = time.time()\n",
    "\n",
    "    accuracy = round(accuracy_score(labels, predictions), 3)\n",
    "    precision = round(precision_score(labels, predictions, average='macro'), 3)\n",
    "    recall = round(recall_score(labels, predictions, average='macro'), 3)\n",
    "    latency = round((end - start) * 1000, 1)  # milliseconds\n",
    "\n",
    "    print(f\"{type(model.named_steps['model']).__name__} -- \"\n",
    "          f\"Accuracy: {accuracy} / Precision: {precision} / Recall: {recall} / Latency: {latency}ms\")\n",
    "\n",
    "df = pd.read_csv(“processed_mpg_data.csv”)\n",
    "\n",
    "label_encoder = LabelEncoder() df\\[‘mpg_class’\\] =\n",
    "label_encoder.fit_transform(df\\[‘mpg_class’\\])\n",
    "\n",
    "X = df.drop(columns=\\[‘mpg_class’\\]) y = df\\[‘mpg_class’\\]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split( X, y,\n",
    "test_size=0.4, random_state=42, stratify=y ) X_val, X_test, y_val,\n",
    "y_test = train_test_split( X_temp, y_temp, test_size=0.5,\n",
    "random_state=42, stratify=y_temp )\n",
    "\n",
    "print(“Train size:”, X_train.shape\\[0\\]) print(“Validation size:”,\n",
    "X_val.shape\\[0\\]) print(“Test size:”, X_test.shape\\[0\\])\n",
    "\n",
    "rf_pipeline = Pipeline(\\[ (‘scaler’, StandardScaler()), (‘model’,\n",
    "RandomForestClassifier(random_state=42))\\])\n",
    "\n",
    "rf_params = { ’model\\_\\_n_estimators’: \\[50, 100, 200\\],\n",
    "’model\\_\\_max_depth’: \\[None, 4, 8, 12\\], ’model\\_\\_min_samples_split’:\n",
    "\\[2, 5, 10\\] }\n",
    "\n",
    "rf_cv = GridSearchCV(rf_pipeline, rf_params, cv=5, scoring=‘accuracy’)\n",
    "rf_cv.fit(X_train, y_train)\n",
    "\n",
    "print(“Forest Tuning Results:”) print_results(rf_cv)\n",
    "\n",
    "joblib.dump(rf_cv.best_estimator\\_, ‘RF_model_mpg.pkl’)\n",
    "\n",
    "print(“Set Results (Random Forest):”) rf_best = rf_cv.best_estimator\\_\n",
    "evaluate_model(rf_best, X_val, y_val)\n",
    "\n",
    "print(“Set Results (Random Forest):”) evaluate_model(rf_best, X_test,\n",
    "y_test)\n",
    "\n",
    "rf_predictions = rf_best.predict(X_test) print(“Report (Random Forest on\n",
    "Test Set):”) print(classification_report(y_test, rf_predictions,\n",
    "target_names=label_encoder.classes\\_))\n",
    "\n",
    "cm = confusion_matrix(y_test, rf_predictions) plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=‘d’, cmap=‘Blues’,\n",
    "xticklabels=label_encoder.classes\\_,\n",
    "yticklabels=label_encoder.classes\\_) plt.xlabel(“Predicted”)\n",
    "plt.ylabel(“Actual”) plt.title(“Confusion Matrix - Random Forest (Test\n",
    "Set)”) plt.show()\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier( n_estimators=200, max_depth=6, \\# Reduce\n",
    "depth to avoid overfitting min_samples_split=5, max_features=‘sqrt’, \\#\n",
    "Randomly select subset of features per split random_state=42 )"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
